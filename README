Copyright 2012-2013 Johns Hopkins University HLTCOE. All rights reserved.
This software is released under the 2-clause BSD license.
See LICENSE in the project root directory.

###########################################################################
##                                 REBAR                                 ##
##-----------------------------------------------------------------------##
##                         Introduction to Rebar                         ##
###########################################################################

1. Introduction
~~~~~~~~~~~~~~~
"Rebar" is a framework for developing and combining language
processing and graph-analysis tools.  At Rebar's core is a set of data
structures that provide a common standard format that can be used to
communicate between tools.  These data structures are defined using a
protobuf [1] specification, which is intended to grow over time as we
add support for additional types of analytic tools.  To store and
organize these protobuf data structures, Rebar defines two kinds of
"data collection":

  * Corpus: consists of a set of Communication protobuf objects.  Each
    Communication protobuf object encodes a single piece of linguistic
    content (such as an email, phone call, or newspaper article).

  * Graph: consists of a set of Vertex and Edge protobuf objects.
    Each Graph is used to encode a combined communication graph and
    knowledge graph, where vertices correspond to people,
    communication channels, and communication instances; and edges are
    used to store relations between those vertices.  Additional vertex
    types (such as organizations or locations) may be added in the
    future.

1.1. Partitioned Protobuf Objects
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Within these data collections, Rebar stores each protobuf object in a
"partitioned" format, with the output of each tool stored in a
separate location (called a "Stage").  When reading from a data
collection, the user specifies which Stages they want to read, and
Rebar automatically combines the selected Stages into a single
protobuf object.  This "partitioned" format has several advantages:

  * Multiple Configurations: A single tool can be run multiple times
    with different configurations, and the results can be stored as
    separate Stages.  These Stages can co-exist side-by-side, making
    it easier to run experiments that compare the effect of different
    parameters on downstream tasks.

  * Efficiency: When a tool is run, it only needs to read the
    annotations that are important for it to run; any other
    annotations can be ignored.

  * Asynchronous Tools: Multiple tools may be run at the same time,
    without having to worry about interfering with one another.
    Additionally, the overal workflow can be arranged in a "lattice"
    rather than a linear "pipeline."

  * Simplified Interface for Tools: Individual tools will know exactly
    what their input will look like, since they can control which
    stages they read.  In other words, tools don't need to worry about
    "ignoring" the output of other tools that are not relevant or
    "passing through" information that is not relevant to the tool.

  * Conflicting Theories: Segregating the output of different tools
    makes it easier to support multiple theories about the correct
    values of annotations.  For example, we might run two parallel
    pipelines of tools that make different assumptions about
    tokenization, and we would only need to worry about merging those
    results at the ends of the pipeline.  And if those end results do
    not contain any direct pointers to the tokenizations, then they
    can be merged fairly trivially.

1.2. Monotonic Modifications
~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In order to simplify the interactions between tools, Rebar requires
that all modifications to the protobuf data structures be "monotonic":
in other words, tools may add new values for fields, but may not
change or delete existing field values.  

However, you can do something very similar to changing (or deleting)
existing values by building a tool that reads in the values generated
by a stage, and writes a new stage containing similar but "modified"
versions of these values.  For example, a parse-tree-repair tool might
read in a Stage containing parse trees, and output a new Stage whose
parse-tree values are based on the input parses, but repair some
issues or problems with those parses.  Downstream tools could then
decide whether they wish to use the original parse Stage, or the
"repaired" parse Stage (or both).  But crucially, the original parse
Stage remains intact, and the output of any tool that have been run
using that original parse Stage remains valid.

1.3. Stage Definitions
~~~~~~~~~~~~~~~~~~~~~~
Each "Stage" records a related set of incremental monotonic changes to
the elements of a Rebar data collection.  Typically, you will generate
a new Stage each time you run a given tool.  However, some tools may
choose to divide their output into multiple stages, to allow for
selective loading of their output -- for example, an analytic can generate
different stages to hold its zoning output, tokenization output,
parsing output, named entity output, coreference output, etc.

Each stage is uniquely defined (within a data collection) by a name
string and a version string.  Typically, the stage name is used to
identify the tool that was run and the type of output generated (eg
"jerboa_tokens", "textnorm", or "serif_parses").  The version is used
to distinguish the output generated by different versions of the tool,
or by different configuration settings or input stages.

In addition to a stage name and a stage version, each Stage is
automatically assigned a positive integer identifier, known as its
StageId.  These identifiers are generated in ascending order, and a
given Stage may only depend on Stages whose StageId are less than its
own StageId.  This ensures that there are no dependency cycles, and
simplifies the process of loading and merging stages.

When a new Stage is created, you must specify a "description" string
for the new stage.  This string is not used programatically in any
way, but can be very helpful in keeping track of information such as
the configuration parameters that were used to generate a given Stage.

1.3.1. Stage Dependencies
~~~~~~~~~~~~~~~~~~~~~~~~~
The dependencies of a stage can be divided into three categories:

  * "Input dependencies" are the information sources that were used to
    construct the stage.  These can include earlier stages, as well as
    external resources or annotations.  We do not currently record
    the input dependencies of a stage (though you are encouraged to
    document them as appropriate in the stage's description string).

  * "Required dependencies" are stages that the new stage adds
    information to.  These are the stages that must be loaded in order
    to access the information contained in the stage.  For example, if
    a tokenizer adds new Tokenizations to a Sentence, then the stage
    that added that Sentence to the communication is a required
    dependency to the tokenizer's stage.  The required dependencies of
    a stage must be explicitly declared when the stage is created.  It
    is an error for a stage to add information to a protobuf object
    that was not loaded by one of its required dependency stages.

  * "Reference dependencies" (aka "recommended dependencies") are
    stages that the new stage adds pointers to.  (See Section 2.1
    below for information on how pointers are encoded in protobuf.)
    When a stage is loaded, it will not be possible to follow these
    pointers unless the stage's reference dependencies are also
    loaded.  We do not currently record the reference dependencies of
    a stage (though we may decide to do so in the future).

In general, the required dependencies and reference dependencies of a
stage are a subset of its input dependencies.  When declaring the
dependencies of a new stage, you should generally list only the
required dependencies, and *not* all of the input dependencies.  For
example, a language ID system might use a tokenization Stage as an
input dependency, but its output stage should not declare that
tokenization stage as a dependency, since the language ID annotation
is attached to the Communication object, and can be used without
loading the tokenization.  Similarly, a Stage that uses parse trees to
help make coreference decisions would not need to include the parse
stage in its dependency list.

Required dependencies are implicitly transitive.  In other words, if
stage A is a required dependency of stage B, and stage B is a required
dependency of stage C, then stage A is a required dependency of stage
C.

1.3.2. Private and Public Stages
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Each Stage may be either "private" or "public".  All stages are
private when they are first created.  This indicates that they should
not yet be used as dependencies for other stages, and that they may be
deleted without warning.  Once a Stage is ready to be used by
downstream tools, it should be made public.  This can be done from the
command line using::

  % python python/rebar2/corpus.py CORPUS mark-stage-public STAGE

or programmatically using the markStagePublic() method.  Deleting or
modifying a public stage is strongly discouraged, since it may
invalidate any stages that depend on it.

2. Protobuf Definitions Overview
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Rebar defines three "top-level" protobuf objects, which are stored in
a partitioned format in the data collections: Communication, Vertex,
and Edge.  The following three sections give an overview of what each
of these data structures currently contains.

The full protobuf definitions for these data types can be found in the
file "$REBAR_HOME/proto/rebar2/rebar.proto".  You can view an HTML
page containing documentation for these data structures by first
running "make doxygen" in the "$REBAR_HOME" directory, and then
opening "$REBAR_HOME/doc/proto/api/index.html".

2.1. Identifiers and Pointers
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Many of the protobuf objects defined by rebar include a field named
"uuid" containing a Universally Unique Identifier (UUID), which can be
used by related protobuf objects to "point at" that object.  UUIDs are
randomly generated, and are drawn from a sufficiently large space of
possible identifiers that the chance of collision (assigning the same
id to two different objects) is virtually zero.  UUIDs are encoded
using a simple protobuf type named "UUID."

The rebar protobuf objects also contain several other identifier types
(including edge identifiers, token identifiers, and parse constituent
identifiers) that can be used to "point at" objects.  These will be
discussed below.

2.2. Communication
~~~~~~~~~~~~~~~~~~
Each "Communication" protobuf object corresponds to a single
communication instance, including both first-person communications (eg
emails or phone calls) and third-person communications (e.g. news
articles).  Each Communication is tagged with a "communication_id"
that is unique within the corpus.  

The definition of what counts as a single communication is left up to
the user, though currently each communication must have a single
unique underlying "text" or "audio" field.  Typical examples of
individual communications include: an email; a phone call; a tweet; a
newspaper article.  We typically choose *not* to aggregate
communications into larger units (e.g., a collection of related
tweets, or a complete email thread).

The "raw" content of the communication is stored in either the 'text'
field or the 'audio' field (depending on its type).  Automatically
derived information is stored in a variety of "annotation" fields,
each of which provides some piece of analysis for the communication.
Annotation objects always define the following two fields:

  * uuid: A unique identifier for the annotation
  * metadata: Information about the tool that generated this annotation.

Annotations are always stored in "repeated" fields, i.e. fields that
may be given multiple values.  This can be used to store competing
theories for a given annotation.  For example, the field
"Communication.language_id", which stores information about the
language(s) used in a communication, may contain multiple
LanguageIdentification objects, reflecting the output of multiple
language-id systems.  In some contexts, the repeated annotation fields
can also be used to store multiple *complementary* theories.  For
example, as we'll se below, a sentence's "tokenization" annotation
field can be used to record not just the basic token sequence of a
sentence, but also the output of machine translation or text
normalization systems.

2.2.1. Communication: Block Structure
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The Communication's "section_segmentation" annotation field is used to
record theories about the block structure of the document -- i.e.,
about how it should be divided into sections or paragraphs or other
super-sentential units.  Each section_segmentation consists of a list
of sections; and each section uses a "sentence_segmentation"
annotation field to record theories about how the section is further
subdivided into sentences.  Finally, each sentence in a
sentence_segmentation contains information about an individual
sentence.  This three-level block structure (Communication->
Section->Sentence) was chosen because it is expressive enough to
encode most types of analysis we perform, but simpler and more
consistent than using an arbitrary hierarchical structure.  Support
for arbitrary hierarchical block structure could be added in the
future if desired [4].

2.2.2. Communication: Sentences
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The "Sentence" protobuf object contains information about a single
sentence.  An important annotation on Sentence objects is the
"tokenization" field, which records theories about how the sentence
can be encoded as a list (or lattice) of Tokens, where each Token has
a unique identifier and a text string.  This text string need not be a
substring of the sentence's original source text.  

2.2.3. Communication: Tokenizations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The "tokenization" annotation field is used in a variety of ways,
reflecting the fact that there are various ways to define the tokens
that make up a sentence:

  1. A Tokenization can be used to divide the text of a Sentence that
     comes from a text source into individual words and punctuation
     marks.  In this case, the tokens' text strings will usually be
     substrings of the source text, or perhaps normalized substrings.

  2. A Tokenization can be used to store the output of an automated
     speech recognition (ASR) system.  In this case, there is no
     source text (just a source audio file), but the tokens will
     typically correspond with specific pieces of the audio file
     (indicated by start/end offset times).

  3. A Tokenization can be used to store a normalized (or "corrected")
     version of a more basic tokenization that is easier for
     downstream tools to process.  For example, a twitter token
     normalization stage might generate a new tokenization that
     expands common abbreviations and corrects typos.  In this case,
     the tokens only roughly correspond with the source text.

  4. A Tokenization can be used to store a translated version of the
     source-langauge tokenization.  In this case, individual tokens
     may not correspond with any particular substring in the source
     source text.

By encoding all of these different tokenization values using the same
field, we make it easy for downstream tools to be configured to ingest
whichever tokenization version makes most sense.  For example, we
could run a relation finder on a translated version of a sentence by
simply using a machine translation system's output stage as its input
-- the relation finder itself does not need to know that it is working
on a translation.

2.2.4. Communication: Entities
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
An "entity" is a concrete referent that can be "mentioned" in a
communication.  Rebar's definition of "entity" is somewhat more broad
than the defintion used in some prior work (such as the "Automated
Content Extraction (ACE)" framework).  For example, Rebar entities
include:

  * ACE-style entities: people, organizations, locations, etc
  * ACE-style values: monetary amounts, URLs, ages, etc.
  * "Named entities"

Each communication contains two top-level fields that can be used to
record information about the entities that are mentioned:

  * entity_mention_set: Contains a collection of "EntityMention"
    protobuf objects, each annotating information about a single
    mention of some entity.

  * entity_set: Contains a collection of "Entity" protobuf objects,
    which group together coreferent entity mentions (i.e., the entity
    mentions that are used to mention the same concrete referent).

Each EntityMention contains fields for:

  * The referent entity's type (e.g., person, organization, etc.)
  * The phrase type (e.g., name, pronoun, common noun, etc.)
  * The set of tokens that are used to mention the entity.

Optional fields may be used to record the entity mention's text, and
its location in the source text or audio.  These fields are typically
redundant with the field that specifies the tokens that make up the
entity; but can be useful because they make it possible to use an
entity mention set without having to load the tokenization that it was
based on.

2.2.5. Communication: Relations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
A "relation" is a typed binary association between two entities.  As
was the case with entities, each communication contains two top-level
fields that can be used to record information about the relations that
are mentioned:

  * relation_mention_set: Contains a collection of "RelationMention"
    protobuf objects, each annotating information about a single
    mention of some relation.

  * relation_set: Contains a collection of "Relation" protobuf
    objects, which group together coreferent relation mentions (i.e.,
    the relation mentions that mention the same relation).

Each RelationMention contains fields for:

  * The relation type, which corresponds to a binary predicate
    (employer, parent, etc).

  * Pointers to the two entity mentions that are associated by this
    relation.  These are recorded in fields named "lhs" (for
    left-hand-side mention) and "rhs" (for right-hand-side mention),
    indicating which side of the relation type predicate each entity
    belongs on.

  * A text string.  Currently, the semantics of this field are
    somewhat underspecified.  It might be used to record the text of
    the entire relation mention, or just the text of some set of
    "anchor words".  This may be further specified in the futre,
    and/or additional fields may be added.

The fields in Relation are similar, except that the "lhs" and "rhs"
fields point to Entities, rather than to EntityMentions.

2.2.6. Communication: Events
~~~~~~~~~~~~~~~~~~~~~~~~~~~~
An "event" consists of an event type and zero or more arguments, where
each argument consists of a role label and a value.  As with entities
and relations, each communication contains two top-level fields that
can be used to record information about the events that are mentioned:

  * event_mention_set: Contains a collection of "EventMention"
    protobuf objects, each annotating information about a single
    mention of some event.

  * event_set: Contains a collection of "Event" protobuf
    objects, which group together coreferent event mentions (i.e.,
    the event mentions that mention the same event).

2.2.7. Communication: Private States
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Information about internal states of people, such as sentiments and
beliefs, can be encoded using the "private state" protobuf objects.
In particular, each communication contains two top-level fields that
can be used to record information about these private states:

  * private_state_mention_set: Contains a collection of
    "PrivateStateMention" protobuf objects, each annotating
    information about a single mention of some private state.  This
    includes both explicit mentions and implicit information (e.g.,
    information about a person's sentiment towards some target might
    be conveyed implicitly by the choice of adjectives that modify
    that target).

  * private_state_set: Contains a collection of "PrivateState"
    protobuf objects, which group together coreferent private state
    mentions (i.e., private state mentions that provide information
    about the same private state).

2.2.8. Communication: Knowledge Graph
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Each communication contains a single "knowledge_graph" field,
consisting of a set of vertices and edges between those vertices.
This graph is used to store graph-level knolwedge that is extracted
from the communication, prior to the "vertex linking," which is
responsible for combining the individual knowledge graphs that are
built for each communication into a Graph data collection.  See 
section 2.3 (below) for more information about vertices and edges.

2.3. Graphs
~~~~~~~~~~~
A "knowledge graph" (or "graph") is a collection of attributed
vertices, connected by attributed edges.  Each vertex corresponds with
a single "knowledge object," where the list of possible knowledge
objects types is defined by the enumerated type "Vertex.Kind".
Currently, rebar supports the following vertex types:

  * PERSON: A vertex that represents a unique individual.

  * COM_CHANNEL: A vertex that represents a single "channel" that can
    be used for communication.  Communication channels are usually
    defined based on unique identifiers such as email addresses or
    twitter handles.

  * COMMUNICATION: A vertex that represents a single communication
    event.  These will typically correspond exactly with Communication
    objects in the corpus data collections (and will contain a pointer
    to the corresponding Communication object).

Other vertex types (such as ORGANIZATION, LOCATION, etc) may be added
in the future.  In general, we need to have a vertex type for any kind
of "knowledge object" that we might want to build a profile for.

There are two types of knowledge graph: 

  * "Communication Knowledge Graphs" are stored in the
    "knowledge_graph" field of each communication, and are used to
    store information that has been extracted from the communication
    itself.

  * "Distributed Knowledge Graphs" are top-level data collections that
    are used to merge information from the individual communications'
    knowledge graphs.

2.3.1. Vertices
~~~~~~~~~~~~~~~
Each vertex is uniquely identified by a UUID, which is used to index
the vertex when it is stored in a distributed knowledge graph.  These
UUIDs are also used by edges to point at the two vertices they
connect.

The Vertex protobuf type also defines a set of "attribute" fields,
which can be used to label the vertex.  These are discussed below (in
2.3.3).

Finally, the Vertex type defines a field named "neighbor" that
contains a list of the UUIDs of all vertices that are connected to the
vertex by an edge.  This field is treated specially by the Rebar
system, and is automatically updated.  In particular, you do not need
to explicitly add vertices to the "neighbor" field; they will be added
automatically whenever you add an edge.  Furthermore, whenever a
vertex is loaded, duplicates are automatically removed from this list.

2.3.2. Edges
~~~~~~~~~~~~
Graph edges themselves are undirected; however, each edge contains
three collections of attributes: one for undirected attributes, and
one for directed attributes in each direction.  Rebar defines two
identifier types for edges:

  * "EdgeId" identifies a single (undirected) edge.  It consists of a
     pair of vertex UUIDs, "v1" and v2", which are stored in a
     normalized format where v1 is the UUID with the lesser value, and
     v2 is the UUID with the greater value.

  * "DirectedEdgeId" identifies an edge and an associated direction.
    It consists of a pair of vertex UUIDs, "src" and "dst", pointing
    to the source vertex and destination vertex, respectively.

Note that there are two possible DirectedEdgeId values corresponding
with each possible EdgeId value, corresponding to the two possible
edge directions.

The protobuf "Edge" type consists of an (undirected) EdgeId, along
with three fields used to specify the edge's attributes:

  * "undirected": A protobuf "UndirectedAttributes" object containing
    the edge's undirected attributes.

  * "v1_to_v2": A protobuf "DirectedAttributes" object containing
    directed attributes whose src=v1 and dst=v2.

  * "v2_to_v1": A protobuf "DirectedAttributes" object containing
    directed attributes whose src=v2 and dst=v1.


2.3.3. Attributes
~~~~~~~~~~~~~~~~~~~
Both Vertices and Edges contain various "attribute" fields that can be
used to add information about each vertex or edge.  In the case of the
protobuf Vertex type, these attributes are all defined as direct
fields on the Vertex object.  In the case of the protobuf Edge type,
these attributes are defined as fields on the UndirectedAttributes and
DirectedAttributes objects owned by the edge.

Attribute fields are always "repeated" -- i.e., it is always possible
to have multiple theories about the value of an attribute.  The type 
for each attribute object defines the following fields:

  * value: The value assigned to this attribute.
  * uuid: A unique identifier for this attribute.
  * metadata: Information about what tool generated this attribute.
  * digest: Analytic-private information about the attribute.

The Rebar protobuf specification defines a variety of different
attribute types, reflecting the different possible value types that
attributes can take.  Some examples of rebar attribute types are:

   * BooleanAttribute -- stores attributes whose value is a boolean.

   * StringAttribute -- stores attributes whose value is a string.

   * StringFloatMapAttribute -- stores attributes whose value is a
     mapping from strings to floats.

   * VertexKindAttribute -- stores attributes whose value is a
     Vertex.Kind value.

It is worth noting that the "kind" of a vertex is stored as an
attribute, meaning that it is possible to have multiple theories about
what kind of object a given vertex corresponds to.  We expect this to
be necessary as we add support for new vertex types (e.g,
ORGANIZATION).

2.4. Protobuf Conventions
~~~~~~~~~~~~~~~~~~~~~~~~~
* Times and dates are encoded as unix time UTC (i.e., seconds since
  January 1, 1970), stored in a double field.

* Human languages are encoded using ISO 639-3 three-letter codes,
  stored in a string.  Other language code formats, such as ISO 639-1
  two-letter codes [2] and Ethnologue SIL codes [3] should be mapped
  to the corresponding ISO 639-3 code.  Two special codes should be
  noted:

      und: Language is undetermined
      zxx: No linguistic content

3. Java API
~~~~~~~~~~~
The Rebar Java interface is contained in the Java package
"edu.jhu.hltcoe.rebar2".  The class "Rebar" contains all of the
automatically-generated protobuf classes, such as
"Rebar.Communication", "Rebar.Tokenization", and "Rebar.Vertex."
These protobuf objects are all immutable, and are typically
constructed using "builders."  See the protobuf documentation for more
details about how to read data from them and how to create new
protobuf objects.

3.1. Indexed Wrappers
~~~~~~~~~~~~~~~~~~~~~
In order to make the protobuf objects easier to work with, Rebar also
defines several indexed "wrapper" classes, such as
"IndexedCommunication," "IndexedTokenization," and "IndexedVertex".
These wrapper classes serve several purposes:

  * Pointer lookup: The indexed-wrapper classes automatically create
    indices that can be used to find the target of protobuf pointer
    types (such as UUIDs, EdgeIds, and TokenRefs).  These indices are
    created on-demand.

  * Mutation tracking: While Java protobuf objects themselves are
    immutable, the indexed wrapper classes are not.  In particular,
    they define methods (such as "addTokenization()") that can be used
    to add new information to the protobuf objects.  When called,
    these methods replace the wrapped protobuf object with a new
    protobuf object with the requested change.  Any related indexed
    wrapper classes that are affected by the change are also updated.
    The change itself is recorded, which is used to record a stage's 
    modifications using the "partitioned" protobuf format.

    In addition to specialized methods such as "addTokenization", the
    generic methods "addField" and "setField" may be used to modify
    any protobuf field contained (directly or indirectly) by an
    indexed wrapper.

  * Helper methods: The indexed-wrapper classes provide methods that
    can be used to perform common operations (such as getting the most
    likely sequence of tokens from a token lattice).  Some of these
    methods create additional internal indices.

Currently, indexed-wrapper classes are defined for the following
Protobuf types:

  * IndexedCommunication
  * IndexedSectionSegmentation
  * IndexedSection
  * IndexedSentenceSegmentation
  * IndexedSentence
  * IndexedTokenization
  * IndexedKnowledgeGraph
  * IndexedVertex
  * IndexedEdge

The IndexedEdge wrapper adds a "direction" to the underlying edge
(stored using a Rebar.DirectedEdgeId.Direction value -- V1_TO_V2 or
V2_TO_V1).  This makes it easier to access direction-specific edge
information.  In particular, the IndexedEdge class defines the
following direction-dependent methods:

  * indexed_edge.getDirection()
  * indexed_edge.getSrc()
  * indexed_edge.getDst()
  * indexed_edge.getSrcUUID()
  * indexed_edge.getDstUUID()
  * indexed_edge.getForwardAttributes()
  * indexed_edge.getBackwardAttributes()

The method IndexedEdge.reversed() returns a new IndexedEdge (wrapping
the same underlying protobuf) with the direction reversed.

3.1.1. ProtoIndex
~~~~~~~~~~~~~~~~~
Under the hood, each indexed wrapper class contains a pointer to a
"ProtoIndex" object, which is used to track changes and hold indices.
A single ProtoIndex is shared by a top-level rebar object and all of
the indexed protobuf objects it contains.  For example, an
IndexedTokenization would use the same ProtoIndex as the
IndexedCommunication that it belongs to.  This allows us to track any
changes made to a top-level object, even if the changes are made using
methods on a contained object.

3.2. Data Collections
~~~~~~~~~~~~~~~~~~~~~
The Java interfaces "Corpus" and "Graph" are used to access the two
types of data collection that rebar defines.  The Corpus.Factory
nested class defines the following static methods:

  * Corpus.Factory.makeCorpus() -- create a new corpus
  * Corpus.Factory.getCorpus() -- connect to an existing corpus
  * Corpus.Factory.corpusExists() -- check if a corpus exists
  * Corpus.Factory.deleteCorpus() -- delete a corpus
  * Corpus.Factory.list() -- get a list of all corpus names

The Graph.Factory nested class defines analagous methods for creating
and accessing graphs.

3.2.1. Stages
~~~~~~~~~~~~~
In order to read or write from a corpus or graph, you will typically
need to create or read one or more "Stage" objects.  These objects
encapsulate Rebar stages, and are returned by the following methods:

  * data_collection.getStage(stageString)
  * data_collection.getStage(name, version)
  * data_collection.getStage(stageId)
  * data_collection.getStages()

For the first version of getStage, if stageString has the form
"NNN:VVV" then NNN is treated as the name and VVV as the version.
Otherwise, stageString is treated as a stage name, and the most
recently created *public* stage with the given name is returned.

To create a new stage, use the following method.  Note that you must
explicitly specify the dependencies of a stage when you create it.

  * data_collection.makeStage(name, version, dependencies, 
                              description, deleteIfExists)

3.2.2. Readers
~~~~~~~~~~~~~~
To read from a data collection, you must first create a "Reader"
object, using the "makeReader()" method.  This method takes a list of
stages, and returns a new Corpus.Reader or Graph.Reader that will
build return values by combining the output of the given set of
stages.

The Corpus.Reader class defines the following methods for reading
communications from a corpus:

  IndexedCommunication loadCommunication(String comid)
  Iterator<IndexedCommunication> loadCommunications(Collection<String> comids)
  Iterator<IndexedCommunication> corpus_reader.loadCommunications()

The Graph.Reader class defines the following methods for reading
vertices and edges:

  IndexedVertex loadVertex(UUID vertexId)
  Iterator<IndexedVertex> loadVertices(Collection<UUID> vertexIds)
  Iterator<IndexedVertex> corpus_reader.loadVertices()

  IndexedEdge loadEdge(EdgeId edgeId)
  IndexedEdge loadEdge(DirectedEdgeId edgeId)
  IndexedEdge loadEdge(IndexedVertex src, IndexedVertex dst)
  IndexedEdge loadEdge(UUID src, UUID dst)
  Iterator<IndexedEdge> loadEdges(Collection<EdgeId> edgeIds)
  Iterator<IndexedEdge> corpus_reader.loadEdges()

3.2.3 Initializers
~~~~~~~~~~~~~~~~~~
When creating a new Corpus, you must first define the set of
Communications that it contains.  These initial Communication objects
will be the "core" objects that get annotated by subsequent stages.
Typically, these "core" objects should be fairly minimal, containing
just a communication id, and possibly the original text or a pointer
to the original audio.  Any annotations (including "ground-truth"
annotations) should be added using proper stages.

To add Communications to a new corpus, use corpus.makeInitializer() to
build a Corpus.Initializer object.  The intializer's
addCommunication() method can then be used to add new communication
objects.

A Corpus.Initializer can also be used to add new communications to an
existing corpus; but this should be done with care, since any stages
that have been run will not contain output for these new
Communications.  It may be easier to simply create a new corpus
containing the new set of communications.  In the future, we may add a
new implementation of the Corpus interface that makes it possible to
define "composite corpora," merging the contents of individual Corpus
objects.

When creating a new Graph, you must similarly define the set of
Vertices that it contains, using a Graph.Initializer.  These initial
vertices should probably *just* contain a UUID; any attributes
(including pointers back to "subsumed" vertices) should be added using
a proper stage.  It is not necessary to pre-specify the set of edges,
because graph edges are created on-demand.

When you are finished using a corpus initializer, you must call its
close() method.  Failure to do so may cause your program to hang
(because a background thread is still running).

3.2.4 Writers
~~~~~~~~~~~~~
Corpus.Write and Graph.Writer objects are used to save any changes
(monotonic modifications) you have made to a Communication, Vertex, or
Edge.  Stage writers are created using the following methods:

  * Corpus.makeWriter(Stage outputStage)
  * Graph.makeWriter(Stage outputStage)

Corpus.Writer defines the method "saveCommunication()", which takes an
IndexedCommunication and saves any changes that have been.  Similarly,
Graph.Writer defines the methods "saveVertex()" and "saveEdge()."  As
was discussed in section 1.1, Rebar will just record the set of
modifications that have been made, and will not store a complete new
copy of the object.

In some cases, a single tool may wish to generate multiple output
stages.  For example, the SERIF analytic uses different output stages
to record the output of its various pipeline stages (parser, relation
finder, coreference, etc.).  To generate multiple output stages, you
should create multiple Corpus.Writers (once for each output stage).
When you call "saveCommunication()", the writer will save all changes
that have been made since the communiation was loaded, *or* since the
last time it was saved.  Thus, you can add one set of annotations to a
communication, then save it using one stage writer; and then add
another set of annotations, and save it using a different stage
writer.

When you are finished using a corpus writer, you must call its close()
method.  Failure to do so may cause your program to hang (because a
background thread is still running).

3.2.4 Utility Classes
~~~~~~~~~~~~~~~~~~~~~
Rebar provides several utility classes.  The most important of these
is the IdUtil class, which defines several useful functions for
creating and manipulating protobuf identifier objects:

  * IdUtil.generateUUID() -- Generate and return a random new UUID

  * IdUtil.uuidToString() -- Convert a UUID to a pretty string repr

  * IdUtil.buildEdgeId() -- Construct a new EdgeID from two UUIDs.
    The UUIDs are automatically put in normalized order.  Overloads
    are also defined for building EdgeIds from pairs of vertices, or
    from DirectedEdgeIds.

  * IdUtil.buildDirectedEdgeId() -- Construct a new DirectedEdgeId
    from two UUIDs.  Overloads are also defined for building EdgeIds
    from pairs of vertices, or from an EdgeId and a Direction.

  * IdUtil.edgeIdIsValid() -- Return true if an EdgeId is valid (i.e.,
    if edgeId.v1<=edgeId.v2)

  * IdUtil.getEdgeDirection() -- Return the direction of a
    DirectedEdgeId.

  * IdUtil.getUUIDOrNull() -- Return the UUID of a given protobuf
    object, or NULL if the given protobuf object's type does not
    define a uuid field.

Another useful utility method is "MathUtil.addLogs(logx, logy)", which
calculates log(e**logx+e**logy) while avoiding overflow errors that
might result from a direct computation of this value.

4. Python API
~~~~~~~~~~~~~
((flesh out this section))

The Rebar python interface is contained in the python package
"rebar2".  The subpackage "rebar2.proto" contains all of the
automatically-generated protobuf classes, such as
"rebar2.proto.Communication", "rebar2.proto.Tokenization", and
"rebar2.proto.Vertex."  Unlike Java, these types are mutable, and are
typically constructed and modified by direct attribute modification.
See the protobuf documentation for more details about how to use
protobuf objects in Python.

The fact that Python protobuf objects are mutable has several
important consequences on how the indexed-wrapper classes are
implemented internally.  (In particular, the way that modifications
are tracked differs from the approach taken in the Java wrapper
classes.)

Another consequence of the fact that Python protobuf objects are
mutable is that they are not hashable.  This is important for rebar
pointer types, such as UUID and EdgeId, because it means that they can
not be used as keys in dictionaries, or as elements in sets.  The
rebar2 package therefore provides several helper classes that can be
used to convert these rebar pointer types into corresponding hashable
immutable values:

  * FrozenUUID
  * FrozenEdgeId
  * FrozenDirectedEdgeId

Other than these differences, the python interface is very similar to
the Java interface.  In particular, "Corpus" and "Graph" classes are
used to access data collections, via Initializers, Readers and
Writers.  And various indexed-wrappers, such as IndexedCommunication,
IndexedTokenization, and IndexedVertex are used to make the raw
protobuf objects easier to work with.  The following classes
correspond exactly one-to-one in Java and Python:

  * Stage
  * Corpus
    * Corpus.Initializer
    * Corpus.Reader
    * Corpus.Writer
  * Graph
    * Graph.Initializer
    * Graph.Reader
    * Graph.Writer
  * IndexedCommunication
  * IndexedSectionSegmentation
  * IndexedSection
  * IndexedSentenceSegmentation
  * IndexedSentence
  * IndexedTokenization
  * IndexedKnowledgeGraph
  * IndexedVertex
  * IndexedEdge
  * ProtoIndex

5. RPC API
~~~~~~~~~~
((not written yet))


A. Rebar 2.0: What's New
~~~~~~~~~~~~~~~~~~~~~~~~
((not finished yet))

Rebar 2 is different in several significant ways from the original
(preliminary) Rebar implementation.  This section describes the more
significant changes.  It assumes that you are already familiar with
Rebar 1.

* Graphs and now separate top-level data collections, and are now
  longer "part" of a Corpus.  This allows you to define multiple
  Graphs for a given Corpus; and to define a single Graph whose
  information is extracted from multiple Corpora.

* Stage output is now stored in a "partitioned" format, as described
  in the introduction.  I.e., the output of each stage is stored
  separately.  When reading a given corpus or graph, this allows you
  to select exactly which sets of stages you wish to use as input.
  This greatly increases rebar's efficiency.  This should also reduce
  many of the difficulties that people had in trying to select the
  "right" input annotations for their tool, since now you can know
  exactly what fields to expect in your input objects.

* In Java, all protobuf objects now live under a single package,
  jhu.hlt.coe.rebar2.Rebar.

* In Java, we no longer convert protobuf UUIDs to java.util.UUID
  objects; we just use the protobuf UUID objects directly.  

[[todo: list other changes]]


Endnotes & References
~~~~~~~~~~~~~~~~~~~~~
[1] https://developers.google.com/protocol-buffers/docs/overview
[2] http://en.wikipedia.org/wiki/ISO_639-1
[3] http://www.ethnologue.com/codes/
[4] During SCALE 2012, we used an arbitrary hierarchical block
    structure, and it was a cause of confusion in several cases.
